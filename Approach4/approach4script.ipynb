{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fourth approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from gym import Env, spaces\n",
    "\n",
    "font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "\n",
    "\n",
    "class Point(object):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.name = name\n",
    "\n",
    "    def set_position(self, x, y):\n",
    "        self.x = self.clamp(x, self.x_min, self.x_max - self.icon_w)\n",
    "        self.y = self.clamp(y, self.y_min, self.y_max - self.icon_h)\n",
    "\n",
    "    def get_position(self):\n",
    "        return (self.x, self.y)\n",
    "\n",
    "    def move(self, del_x, del_y):\n",
    "        self.x += del_x\n",
    "        self.y += del_y\n",
    "\n",
    "        self.x = self.clamp(self.x, self.x_min, self.x_max - self.icon_w)\n",
    "        self.y = self.clamp(self.y, self.y_min, self.y_max - self.icon_h)\n",
    "\n",
    "    def clamp(self, n, minn, maxn):\n",
    "        return max(min(maxn, n), minn)\n",
    "\n",
    "\n",
    "class Chopper(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Chopper, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.cvtColor(cv2.imread(\"./../graphics/chopper.png\"), cv2.COLOR_BGR2GRAY) / 255.0\n",
    "        # self.icon=gray_img/255.0\n",
    "        self.icon_w = 10\n",
    "        self.icon_h = 10\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))\n",
    "        # self.icon=np.atleast_3d(self.icon)\n",
    "\n",
    "\n",
    "class Bird(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Bird, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.cvtColor(cv2.imread(\"./../graphics/bird.png\"), cv2.COLOR_BGR2GRAY) / 255.0\n",
    "        self.icon_w = 8\n",
    "        self.icon_h = 8\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))\n",
    "        # self.icon=np.atleast_3d(self.icon)\n",
    "\n",
    "\n",
    "class Fuel(Point):\n",
    "    def __init__(self, name, x_max, x_min, y_max, y_min):\n",
    "        super(Fuel, self).__init__(name, x_max, x_min, y_max, y_min)\n",
    "        self.icon = cv2.cvtColor(cv2.imread(\"./../graphics/fuel.png\"), cv2.COLOR_BGR2GRAY) / 255.0\n",
    "        self.icon_w = 8\n",
    "        self.icon_h = 8\n",
    "        self.icon = cv2.resize(self.icon, (self.icon_h, self.icon_w))\n",
    "        # self.icon=np.atleast_3d(self.icon)\n",
    "\n",
    "\n",
    "class ChopperScape(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChopperScape, self).__init__()\n",
    "\n",
    "        # 2D observ space\n",
    "        self.total_reward = 0\n",
    "        self.observation_shape = (84, 84, 3)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=np.zeros(self.observation_shape),\n",
    "                                            high=np.ones(self.observation_shape),\n",
    "                                            dtype=np.float16)\n",
    "\n",
    "        # Actions ranging from 0-3\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.possible_bird_pos = [82, 75, 65, 55, 45, 35, 25, 15, 5]\n",
    "        # Create a canvas to render the environment images upon\n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "        # elements from environment\n",
    "        self.elements = []\n",
    "\n",
    "        self.max_fuel = 1000\n",
    "\n",
    "        self.y_min = int(self.observation_shape[0] * 0.1)\n",
    "        self.x_min = 0\n",
    "\n",
    "        self.y_max = int(self.observation_shape[0] * 0.9)\n",
    "        self.x_max = int(self.observation_shape[1])\n",
    "\n",
    "    def draw_elements_on_canvas(self):\n",
    "        # Init the canvas\n",
    "        self.canvas = np.ones(self.observation_shape)\n",
    "        b, g, r = cv2.split(self.canvas)\n",
    "        # Draw the elements on the canvas\n",
    "        for elem in self.elements:\n",
    "            elem_shape = elem.icon.shape\n",
    "            x, y = elem.x, elem.y\n",
    "            b[y: y + elem_shape[1], x:x + elem_shape[0]] = elem.icon\n",
    "            g[y: y + elem_shape[1], x:x + elem_shape[0]] = elem.icon\n",
    "            r[y: y + elem_shape[1], x:x + elem_shape[0]] = elem.icon\n",
    "        self.canvas = cv2.merge([r, g, b])\n",
    "\n",
    "        text = 'Fuel Left: {} | Rewards: {}'.format(self.fuel_left, self.ep_return)\n",
    "\n",
    "        # Put the info on canvas\n",
    "        # self.canvas = cv2.putText(self.canvas, text, (2, 5), font,\n",
    "        #                           0.8, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    def has_collided(self, elem1, elem2):\n",
    "        x_col = False\n",
    "        y_col = False\n",
    "\n",
    "        elem1_x, elem1_y = elem1.get_position()\n",
    "        elem2_x, elem2_y = elem2.get_position()\n",
    "\n",
    "        if 2 * abs(elem1_x - elem2_x) <= (elem1.icon_w + elem2.icon_w):\n",
    "            x_col = True\n",
    "\n",
    "        if 2 * abs(elem1_y - elem2_y) <= (elem1.icon_h + elem2.icon_h):\n",
    "            y_col = True\n",
    "\n",
    "        if x_col and y_col:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        # Flag that marks the termination of an episode\n",
    "        done = False\n",
    "\n",
    "        # Assert that it is a valid action\n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "\n",
    "        # Decrease the fuel counter\n",
    "        self.fuel_left -= 1\n",
    "\n",
    "        # Reward for executing a step.\n",
    "        reward = -0.1\n",
    "\n",
    "        # apply the action to the chopper\n",
    "        if action == 0:\n",
    "            self.chopper.move(0, 2)\n",
    "        elif action == 1:\n",
    "            self.chopper.move(0, -2)\n",
    "        elif action == 2:\n",
    "            self.chopper.move(2, 0)\n",
    "        elif action == 3:\n",
    "            self.chopper.move(-2, 0)\n",
    "        elif action == 4:\n",
    "            self.chopper.move(0, 0)\n",
    "\n",
    "        # Spawn a bird at the right edge with prob 0.01\n",
    "        if random.random() < 0.019:\n",
    "            # Spawn a bird\n",
    "            spawned_bird = Bird(\"bird\".format(self.bird_count), self.x_max, self.x_min, self.y_max, self.y_min)\n",
    "            self.bird_count += 1\n",
    "\n",
    "            # Compute the x,y co-ordinates of the position from where the bird has to be spawned\n",
    "            # Horizontally, the position is on the right edge and vertically, the height is randomly\n",
    "            # sampled from the set of permissible values\n",
    "            bird_x = self.x_max\n",
    "            bird_y = random.choice(self.possible_bird_pos)\n",
    "            # bird_y = random.randrange(self.y_min, self.y_max)\n",
    "            spawned_bird.set_position(bird_x, bird_y)\n",
    "\n",
    "            # Append the spawned bird to the elements currently present in Env.\n",
    "            self.elements.append(spawned_bird)\n",
    "\n",
    "            # Spawn a fuel at the bottom edge with prob 0.01\n",
    "        if random.random() < 0.015:\n",
    "            # Spawn a fuel tank\n",
    "            spawned_fuel = Fuel(\"fuel\".format(self.bird_count), self.x_max, self.x_min, self.y_max, self.y_min)\n",
    "            self.fuel_count += 1\n",
    "\n",
    "            # Compute the x,y co-ordinates of the position from where the fuel tank has to be spawned\n",
    "            # Horizontally, the position is randomly chosen from the list of permissible values and\n",
    "            # vertically, the position is on the bottom edge\n",
    "            fuel_x = random.randrange(self.x_min + 20, self.x_max - 10)\n",
    "            fuel_y = self.y_max\n",
    "            spawned_fuel.set_position(fuel_x, fuel_y)\n",
    "\n",
    "            # Append the spawned fuel tank to the elements currently present in the Env.\n",
    "            self.elements.append(spawned_fuel)\n",
    "\n",
    "            # For elements in the Ev\n",
    "        for elem in self.elements:\n",
    "            if isinstance(elem, Bird):\n",
    "                # If the bird has reached the left edge, remove it from the Env\n",
    "                if elem.get_position()[0] <= self.x_min:\n",
    "                    self.bird_count -= 1\n",
    "                    self.elements.remove(elem)\n",
    "                else:\n",
    "                    # Move the bird left by 5 pts.\n",
    "                    elem.move(-2, 0)\n",
    "\n",
    "                # If the bird has collided.\n",
    "                if self.has_collided(self.chopper, elem):\n",
    "                    # Conclude the episode and remove the chopper from the Env.\n",
    "                    done = True\n",
    "                    reward = -100\n",
    "                    if self.elements.__contains__(self.chopper):\n",
    "                        self.elements.remove(self.chopper)\n",
    "\n",
    "            if isinstance(elem, Fuel):\n",
    "                # If the fuel tank has reached the top, remove it from the Env\n",
    "                if elem.get_position()[1] <= self.y_min:\n",
    "                    self.elements.remove(elem)\n",
    "                else:\n",
    "                    # Move the Tank up by 5 pts.\n",
    "                    elem.move(0, -2)\n",
    "\n",
    "                # If the fuel tank has collided with the chopper.\n",
    "                if self.has_collided(self.chopper, elem):\n",
    "                    # Remove the fuel tank from the env.\n",
    "                    reward = 1\n",
    "                    if self.elements.__contains__(elem):\n",
    "                        self.elements.remove(elem)\n",
    "\n",
    "                    # Fill the fuel tank of the chopper to full.\n",
    "                    self.fuel_left = self.max_fuel\n",
    "\n",
    "        # Increment the episodic return\n",
    "        # self.ep_return += reward\n",
    "\n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "\n",
    "        # If out of fuel, end the episode.\n",
    "        if self.fuel_left == 0:\n",
    "            done = True\n",
    "\n",
    "        # actual_state = {}\n",
    "        # for e in self.elements:\n",
    "        #     actual_state[abs(e.x - self.chopper.x) + abs(e.y - self.chopper.y)] = (e.x, e.y)\n",
    "\n",
    "        # print(np.argmin(actual_state))\n",
    "        self.ep_return += reward\n",
    "        return self.canvas, reward, done, []\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the fuel consumed\n",
    "        self.fuel_left = self.max_fuel\n",
    "\n",
    "        # Reset the reward\n",
    "        self.ep_return = 0\n",
    "\n",
    "        # Reset the total_rewards\n",
    "        self.total_reward = 0\n",
    "        # Number of birds\n",
    "        self.bird_count = 0\n",
    "        self.fuel_count = 0\n",
    "\n",
    "        # Determine a place to intialise the chopper in\n",
    "        x = random.randrange(int(self.observation_shape[0] * 0.05), int(self.observation_shape[0] * 0.10))\n",
    "        y = 50\n",
    "\n",
    "        # Intialise the chopper\n",
    "        self.chopper = Chopper(\"chopper\", self.x_max, self.x_min, self.y_max, self.y_min)\n",
    "        self.chopper.set_position(x, y)\n",
    "\n",
    "        # Intialise the elements\n",
    "        self.elements = [self.chopper]\n",
    "\n",
    "        # Reset the Canvas\n",
    "        self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "        # Draw elements on the canvas\n",
    "        self.draw_elements_on_canvas()\n",
    "\n",
    "        # return the observation\n",
    "        # actual_state = []\n",
    "        # for e in self.elements:\n",
    "        #     actual_state.append((e.name, e.x, e.y))\n",
    "\n",
    "        return self.canvas\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        assert mode in [\"human\", \"rgb_array\"], \"Invalid mode, must be either \\\"human\\\" or \\\"rgb_array\\\"\"\n",
    "        if mode == \"human\":\n",
    "            cv2.imshow(\"Game\", self.canvas)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "        elif mode == \"rgb_array\":\n",
    "            return self.canvas\n",
    "\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def get_action_meanings(self):\n",
    "        return {0: \"Right\", 1: \"Left\", 2: \"Down\", 3: \"Up\", 4: \"Do Nothing\"}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Conv1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import adam_v2\n",
    "from av7.deep_q_learning import DQN, DDQN, DuelingDQN\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def build_model(state_space_shape, num_actions, learning_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, (8, 8), (4, 4), input_shape=state_space_shape, activation='relu'))\n",
    "    model.add(Conv2D(32, (4, 4), (2, 2), activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "\n",
    "    model.compile(adam_v2.Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float16\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = ChopperScape()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(84, 84, 3)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "learning_rate = 1e-2\n",
    "state_space_shape = env.observation_space.shape\n",
    "print(num_actions)\n",
    "print(state_space_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "max_steps = 1000\n",
    "num_episodes = 10000\n",
    "batch_size = 16\n",
    "memory_size = 4096\n",
    "epsilon = 0.7\n",
    "discount_factor = 0.99\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m target_model \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_model\u001B[49m(state_space_shape, num_actions, learning_rate)\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(state_space_shape, num_actions, learning_rate)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "target_model = build_model(state_space_shape, num_actions, learning_rate)\n",
    "model = build_model(state_space_shape, num_actions, learning_rate)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "agent = DQN(state_space_shape, num_actions, model, target_model, learning_rate, discount_factor, batch_size,\n",
    "            memory_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "100%|██████████| 10000/10000 [3:12:39<00:00,  1.16s/it]      \n"
     ]
    }
   ],
   "source": [
    "episode_summary = []\n",
    "for episode in tqdm(range(1, num_episodes + 1)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while not done and steps < max_steps:\n",
    "        # t = time.time()\n",
    "        # print(steps)\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        agent.update_memory(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "    agent.train()\n",
    "    agent.update_target_model()\n",
    "    # epsilon -= decay\n",
    "    # print(epsilon)\n",
    "    if steps == max_steps:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"MAX_STEPS\"])\n",
    "    elif total_reward >= 7:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"WON\"])\n",
    "    elif env.fuel_left == 0:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"NO_FUEL\"])\n",
    "    else:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"DIED\"])\n",
    "\n",
    "    # print(env.ep_return)\n",
    "    if episode % 2000 == 0:\n",
    "        agent.save(\"saved\", episode)\n",
    "\n",
    "df = pd.DataFrame(episode_summary, columns=['Episode', 'Score', 'Steps', 'Remaining_F,uel', 'Cause'])\n",
    "df.to_csv(\"DQN_TRAINING_APP4\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "agent = DDQN(state_space_shape, num_actions, model, target_model, learning_rate, discount_factor, batch_size,\n",
    "             memory_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "100%|██████████| 10000/10000 [1:49:05<00:00,  1.53it/s] \n"
     ]
    }
   ],
   "source": [
    "episode_summary = []\n",
    "for episode in tqdm(range(1, num_episodes + 1)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while not done and steps < max_steps:\n",
    "        # t = time.time()\n",
    "        # print(steps)\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        agent.update_memory(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "    agent.train()\n",
    "    agent.update_target_model()\n",
    "    # epsilon -= decay\n",
    "    # print(epsilon)\n",
    "    if steps == max_steps:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"MAX_STEPS\"])\n",
    "    elif total_reward >= 7:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"WON\"])\n",
    "    elif env.fuel_left == 0:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"NO_FUEL\"])\n",
    "    else:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"DIED\"])\n",
    "\n",
    "    # print(env.ep_return)\n",
    "    if episode % 2000 == 0:\n",
    "        agent.save(\"saved\", episode)\n",
    "\n",
    "df = pd.DataFrame(episode_summary, columns=['Episode', 'Score', 'Steps', 'Remaining_Fuel', 'Cause'])\n",
    "df.to_csv(\"DDQN_TRAINING_APP4\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "layers = [Conv2D(16, (8, 8), (4, 4), input_shape=state_space_shape, activation='relu'),\n",
    "          Conv2D(32, (4, 4), (2, 2), activation='relu'),\n",
    "          Flatten(),\n",
    "          Dense(256, activation='relu'),\n",
    "          Dense(128, activation='relu'),\n",
    "          Dense(num_actions, activation='relu')\n",
    "          ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DuelingDQN(state_space_shape, num_actions, learning_rate, discount_factor, batch_size, memory_size)\n",
    "\n",
    "agent.build_model(layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "100%|██████████| 10000/10000 [1:35:32<00:00,  1.74it/s] \n"
     ]
    }
   ],
   "source": [
    "episode_summary = []\n",
    "for episode in tqdm(range(1, num_episodes + 1)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    if episode == 1000:\n",
    "        epsilon = 0.5\n",
    "    elif episode == 1500:\n",
    "        epsilon = 0.2\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        # t = time.time()\n",
    "        # print(steps)\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # env.render()\n",
    "        agent.update_memory(state, action, reward, new_state, done)\n",
    "        state = new_state\n",
    "        steps += 1\n",
    "        total_reward += reward\n",
    "    agent.train()\n",
    "    agent.update_target_model()\n",
    "    # epsilon -= decay\n",
    "    # print(epsilon)\n",
    "    if steps == max_steps:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"MAX_STEPS\"])\n",
    "    elif total_reward >= 7:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"WON\"])\n",
    "    elif env.fuel_left == 0:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"NO_FUEL\"])\n",
    "    else:\n",
    "        episode_summary.append([episode, total_reward, steps, env.fuel_left, \"DIED\"])\n",
    "\n",
    "    # print(env.ep_return)\n",
    "    if episode % 2000 == 0:\n",
    "        agent.save(\"saved\", episode)\n",
    "\n",
    "df = pd.DataFrame(episode_summary, columns=['Episode', 'Score', 'Steps', 'Remaining_Fuel', 'Cause'])\n",
    "df.to_csv(\"DUELING_TRAINING_APP4\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "epsilon = 0.0\n",
    "testing_results = []\n",
    "testing_episodes = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mite\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float16\u001B[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = ChopperScape()\n",
    "\n",
    "# env.render()\n",
    "env.reset()\n",
    "# env.render()\n",
    "for i in range(testing_episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    # env.render()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        # action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        # print(action)\n",
    "        # Render the game\n",
    "        # print(obs)\n",
    "        env.render()\n",
    "        if done == True:\n",
    "            if steps == max_steps:\n",
    "                testing_results.append([i, env.ep_return, steps, env.fuel_left, \"MAX_STEPS\"])\n",
    "            elif env.ep_return + steps >= 1500:\n",
    "                testing_results.append([i, env.ep_return, steps, env.fuel_left, \"WON\"])\n",
    "            elif env.fuel_left == 0:\n",
    "                testing_results.append([i, env.ep_return, steps, env.fuel_left, \"NO_FUEL\"])\n",
    "            else:\n",
    "                testing_results.append([i, env.ep_return, steps, env.fuel_left, \"DIED\"])\n",
    "            # testing_results.append([i,total_reward,env.fuel_left])\n",
    "            # print(total_reward)\n",
    "            break\n",
    "# print(df)\n",
    "env.close()\n",
    "\n",
    "rez = pd.DataFrame(testing_results, columns=[\"Episode\", \"Score\", \"Steps\", \"Fuel_Left\", \"Cause\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-121.09588000000005\n",
      "-121.81373000000005\n",
      "-119.47970000000005\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rez1=pd.read_csv(\"DQN_TRAINING_APP4\")\n",
    "rez2=pd.read_csv(\"DUELING_TRAINING_APP4\")\n",
    "rez3=pd.read_csv(\"DDQN_TRAINING_APP4\")\n",
    "\n",
    "print(rez1['Score'].mean())\n",
    "print(rez3['Score'].mean())\n",
    "print(rez2['Score'].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}